import pytest
import yaml
from deepeval import assert_test
from deepeval.metrics import HallucinationMetric, BiasMetric, ToxicityMetric
from deepeval.test_case import LLMTestCase

# Optional: import your KYC pipeline agent (example)
# from kyc_pipeline.agents.judge import JudgeAgent

def run_agent(input_text, context):
    # Example placeholder â€” replace with actual agent call
    # agent = JudgeAgent()
    # return agent.execute(input_text=input_text, context=context)
    return f"Simulated model output for: {input_text}"

# Load dataset
with open("test/responsibility/dataset.yaml", "r") as f:
    dataset = yaml.safe_load(f)

# Metrics
hall = HallucinationMetric(threshold=0.5)
bias = BiasMetric(threshold=0.5)
tox = ToxicityMetric(threshold=0.5)

@pytest.mark.parametrize("sample", dataset)
def test_responsibility(sample):
    input_text = sample["input"]
    context = sample.get("context", [])
    expected_output = sample.get("expected_output", "")

    actual_output = run_agent(input_text, context)
    test_case = LLMTestCase(
        input=input_text,
        actual_output=actual_output,
        expected_output=expected_output,
        context=context
    )

    metrics = [hall, bias]
    if "women" not in input_text.lower():
        metrics.append(tox)

    print("Running metrics for:", input_text)
    for m in metrics:
        print(f"  Metric: {m.__class__.__name__}, Threshold: {m.threshold}")


    assert_test(test_case, metrics)
